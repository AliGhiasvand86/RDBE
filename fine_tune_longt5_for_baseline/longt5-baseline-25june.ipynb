{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8711760,"sourceType":"datasetVersion","datasetId":5226241},{"sourceId":8720192,"sourceType":"datasetVersion","datasetId":5232468},{"sourceId":8736886,"sourceType":"datasetVersion","datasetId":5244949},{"sourceId":8785634,"sourceType":"datasetVersion","datasetId":5281685}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02f3c227b5ad4a09a7b7ca99d5f9c192":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_702f96acc990496baa55ef9afdca254e","IPY_MODEL_cb940873fa3c4a079cc86d49a9bdb476","IPY_MODEL_57822783300d439b855d01ae270f667e"],"layout":"IPY_MODEL_723779cfa0ed469ea08b14aa31027e37"}},"3759318681a1494db21a997ffc7115fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4638669801234adc87d30acaf251ee50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57822783300d439b855d01ae270f667e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4638669801234adc87d30acaf251ee50","placeholder":"â€‹","style":"IPY_MODEL_3759318681a1494db21a997ffc7115fa","value":"â€‡0/8â€‡[00:00&lt;?,â€‡?â€‡examples/s]"}},"5f7fcbfa5f64420e9d2923db58f2ce72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"702f96acc990496baa55ef9afdca254e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7fcbfa5f64420e9d2923db58f2ce72","placeholder":"â€‹","style":"IPY_MODEL_8db5eeed353449bf9523cab48e86d644","value":"Map:â€‡â€‡â€‡0%"}},"723779cfa0ed469ea08b14aa31027e37":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8db5eeed353449bf9523cab48e86d644":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb940873fa3c4a079cc86d49a9bdb476":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_d67e8c957a95473baecadfa8d9652214","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfa65ebc6b9b47b6a0e950ec1af25649","value":0}},"d67e8c957a95473baecadfa8d9652214":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfa65ebc6b9b47b6a0e950ec1af25649":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# @title Install Dependecy\n!pip install groq\n!pip install accelerate==0.27.2\n!pip install datasets\n!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWJwXNxJKbJ9","outputId":"f502dcf6-6e40-4a3b-82b1-100094f158f3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: wandb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.2)\n\nRequirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (8.1.7)\n\nRequirement already satisfied: docker-pycreds>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (0.4.0)\n\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (3.1.43)\n\nRequirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.2.2)\n\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.23.4)\n\nRequirement already satisfied: psutil>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (5.9.8)\n\nRequirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (6.0.1)\n\nRequirement already satisfied: requests<3,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.32.3)\n\nRequirement already satisfied: sentry-sdk>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.6.0)\n\nRequirement already satisfied: setproctitle in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (1.3.3)\n\nRequirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (69.5.1)\n\nRequirement already satisfied: six>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n\nRequirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n\nRequirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n\nRequirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"}]},{"cell_type":"code","source":"# @title Load Dependecy\n# from groq import Groq\nimport pandas as pd\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nfrom datasets import Dataset\nimport tqdm \nfrom sklearn.model_selection import train_test_split","metadata":{"id":"HvXavAWNMZaM","execution":{"iopub.status.busy":"2024-06-25T19:46:14.497192Z","iopub.execute_input":"2024-06-25T19:46:14.497883Z","iopub.status.idle":"2024-06-25T19:46:14.503396Z","shell.execute_reply.started":"2024-06-25T19:46:14.497849Z","shell.execute_reply":"2024-06-25T19:46:14.502332Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # @title Getting Inference from LLAMA\n\n# # load dataset\n# file_path = 'DREsS_New.xlsx'\n# df = pd.read_excel(file_path)\n\n# # set API key\n# api_key = \"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\"\n# client = Groq(api_key=api_key)\n\n# # function to request and get model response\n# def get_model_response(prompt):\n#     completion = client.chat.completions.create(\n#         model=\"llama3-70b-8192\",\n#         messages=[{\"role\": \"user\", \"content\": prompt}],\n#         temperature=0.7,\n#         max_tokens=8192,\n#         top_p=0.8,\n#         stop=None,\n#     )\n#     response = completion.choices[0].message.content\n#     return response\n\n# # select first 10 rows of the dataset\n# df_subset = df.head(10)\n\n# # create new columns for analysis\n# df_subset['content_analysis'] = \"\"\n# df_subset['organization_analysis'] = \"\"\n# df_subset['language_analysis'] = \"\"\n\n# # process each row and get model responses\n# for index, row in df_subset.iterrows():\n#     prompt_base = f\"this is my topic : {row['prompt']} this is my essay : {row['essay']}\"\n\n#     # get analysis for content score\n#     content_prompt = f\"{prompt_base} the content score of this essay is {row['content']}. Explain why it gets this score and provide the reason.\"\n#     content_analysis = get_model_response(content_prompt)\n#     df_subset.at[index, 'content_analysis'] = content_analysis\n\n#     # get analysis for organization score\n#     organization_prompt = f\"{prompt_base} the organization score of this essay is {row['organization']}. Explain why it gets this score and provide the reason.\"\n#     organization_analysis = get_model_response(organization_prompt)\n#     df_subset.at[index, 'organization_analysis'] = organization_analysis\n\n#     # get analysis for language score\n#     language_prompt = f\"{prompt_base} the language score of this essay is {row['language']}. Explain why it gets this score and provide the reason.\"\n#     language_analysis = get_model_response(language_prompt)\n#     df_subset.at[index, 'language_analysis'] = language_analysis\n\n# # save results to a new CSV file\n# output_file_path = 'DREsS_New_with_multiple_analysis.csv'\n# df_subset.to_csv(output_file_path, index=False)\n","metadata":{"_kg_hide-output":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UKBQc-qmLly-","outputId":"11e90db8-4bee-4693-a0fc-8df2e6f5a8af","trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dress-llama-inference-v2-23june/DREsS_LLAMA_Inference_v2_23june.csv')\n\n# Split data in 60:20:20 ratio\ntrain_df, temp_df = train_test_split(df, test_size=0.4, random_state=22)\ntest_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=22)\ntrain_data = Dataset.from_pandas(train_df)\nval_data = Dataset.from_pandas(val_df)\ntest_df= test_df\ntest_data = Dataset.from_pandas(test_df)\nprint(len(train_data))\nprint(len(val_data))\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:46:18.584798Z","iopub.execute_input":"2024-06-25T19:46:18.585737Z","iopub.status.idle":"2024-06-25T19:46:18.953048Z","shell.execute_reply.started":"2024-06-25T19:46:18.585697Z","shell.execute_reply":"2024-06-25T19:46:18.952062Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1187\n396\n396\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\nimport pandas as pd\nfrom datasets import Dataset\n\n\n\n# Constants\nmodel_path = \"google/long-t5-tglobal-base\"\ninput_max_length = 638\ntarget_max_length = 12\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noutput_dir = 'model/'\nnum_train_epochs = 15\nlearning_rate = 3e-4\ntrain_batch_size = 4\neval_batch_size = 4\n\n# Dictionary of score types with descriptions\n\n\n# Prepare the data\ndef create_input_target(row):\n    input_text = f\"{row['prompt']}\\n{row['essay']}\"\n    target_text = f\"content {row['content']}, organization {row['organization']}, language {row['language']}\"\n    return {'inputs': input_text, 'targets': target_text}\n\n# Function to process DataFrame and create inputs and targets\ndef process_dataframe(df):\n    inputs = []\n    targets = []\n    for idx, row in df.iterrows():\n        input_target_pair = create_input_target(row)\n        inputs.append(input_target_pair['inputs'])\n        targets.append(input_target_pair['targets'])\n    return pd.DataFrame({'inputs': inputs, 'targets': targets})\n\n# Assuming train_df, val_df, and test_df are your dataframes\ntrain_df_new = process_dataframe(train_df)\nval_df_new = process_dataframe(val_df)\ntest_df_new = process_dataframe(test_df)\n# Convert to Hugging Face datasets\ntrain_data_new = Dataset.from_pandas(train_df_new)\nval_data_new = Dataset.from_pandas(val_df_new)\ntest_data_new = Dataset.from_pandas(test_df_new)\n\n# Initialize tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n\n# Tokenize data\ndef convert_examples_to_features(example_batch):\n    input_texts = example_batch[\"inputs\"]\n    target_texts = example_batch[\"targets\"]\n\n    input_encodings = tokenizer(list(input_texts), padding=\"max_length\", truncation=True, max_length=input_max_length)\n    target_encodings = tokenizer(list(target_texts), padding=\"max_length\", truncation=True, max_length=target_max_length)\n\n    return {\n        'input_ids': input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n\ntrain_pt = train_data_new.map(convert_examples_to_features, batched=True)\nval_pt = val_data_new.map(convert_examples_to_features, batched=True)\ntest_pt = test_data_new.map(convert_examples_to_features, batched=True)\n\n# Data collator\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Training arguments\ntrainer_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    learning_rate=learning_rate,\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    seed=22,  # Report to wandb\n    load_best_model_at_end= True,\n    save_strategy=\"epoch\",\n    save_total_limit = 2\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=trainer_args,\n    tokenizer=tokenizer,\n    data_collator=seq2seq_data_collator,\n    train_dataset=train_pt,\n    eval_dataset=val_pt\n)\n\n# Train and save the model\ntrainer.train()\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Save test dataset for later use\ntest_data_new.save_to_disk(output_dir + 'test_data')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"JqZ469Kt2Gdj","outputId":"fcea7cef-b206-4ec3-c47e-aca4d6aba09c","execution":{"iopub.status.busy":"2024-06-25T19:46:22.086800Z","iopub.execute_input":"2024-06-25T19:46:22.087310Z","iopub.status.idle":"2024-06-25T20:29:47.398560Z","shell.execute_reply.started":"2024-06-25T19:46:22.087270Z","shell.execute_reply":"2024-06-25T20:29:47.397095Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/851 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded2395264a94771bff168ec4d510ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701206e2f0014662bb29939fef356a3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252a50d071d449ef800784aee4310ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c90c22186eb4f6094e473e85e23b21a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2d7e7c5ccd40aba948a959cd6b4a22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1187 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8e73ed19ed4838b1eb7e1c611a2c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3e61aeb0f8404992eda8098cb3b8e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d098209957364bb983c9c215c75f4fa4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114200777777859, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf1ee9312bb497db076e4a36f805cf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240625_194648-6m9fmf6q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aghiasvandm/huggingface/runs/6m9fmf6q' target=\"_blank\">model/</a></strong> to <a href='https://wandb.ai/aghiasvandm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aghiasvandm/huggingface' target=\"_blank\">https://wandb.ai/aghiasvandm/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aghiasvandm/huggingface/runs/6m9fmf6q' target=\"_blank\">https://wandb.ai/aghiasvandm/huggingface/runs/6m9fmf6q</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4455' max='4455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4455/4455 42:36, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727100</td>\n      <td>0.401534</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.386100</td>\n      <td>0.360860</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.360000</td>\n      <td>0.352484</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.331300</td>\n      <td>0.364025</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.307800</td>\n      <td>0.384431</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.275600</td>\n      <td>0.413887</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.242000</td>\n      <td>0.462136</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.213100</td>\n      <td>0.493181</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.180000</td>\n      <td>0.583126</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.158000</td>\n      <td>0.658359</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.126500</td>\n      <td>0.767338</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.110300</td>\n      <td>0.832329</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.087800</td>\n      <td>1.043066</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.081600</td>\n      <td>1.050527</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.064800</td>\n      <td>1.124050</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a60e7f08fd49269655048483bc20f9"}},"metadata":{}}]},{"cell_type":"code","source":"# def calculate_max_token_length(dataset, max_percentage=0.95):\n#     lengths = []\n\n#     for example in dataset:\n#         input_tokens = tokenizer.encode(example['targets'], truncation=True, max_length=input_max_length)\n#         lengths.append(len(input_tokens))\n\n#     lengths.sort()\n#     max_length_index = int(len(lengths) * max_percentage)\n#     max_token_length = lengths[max_length_index]\n#     print(len(lengths))\n#     print(max_length_index)\n#     return max_token_length\n\n# # Calculate maximum input token length\n# max_input_length = calculate_max_token_length(val_data_new)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:17:25.594226Z","iopub.execute_input":"2024-06-25T19:17:25.594619Z","iopub.status.idle":"2024-06-25T19:17:25.668024Z","shell.execute_reply.started":"2024-06-25T19:17:25.594588Z","shell.execute_reply":"2024-06-25T19:17:25.667016Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"396\n376\n","output_type":"stream"}]},{"cell_type":"code","source":"max_input_length","metadata":{"execution":{"iopub.status.busy":"2024-06-25T19:17:28.825534Z","iopub.execute_input":"2024-06-25T19:17:28.826230Z","iopub.status.idle":"2024-06-25T19:17:28.833074Z","shell.execute_reply.started":"2024-06-25T19:17:28.826193Z","shell.execute_reply":"2024-06-25T19:17:28.832034Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"# import torch\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# # Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ù…Ø¯Ù„\n# checkpoint_dir = \"model/checkpoint-4278\"\n\n# # Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¯Ø± Hugging Face (Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú†ÛŒØ²ÛŒ Ù…Ø«Ù„ \"username/model_name\" Ø¨Ø§Ø´Ø¯)\n# repo_name = \"AliGhiasvand86/epoch_17_load_best_model\"\n\n# # Ù„Ø§Ú¯ÛŒÙ† Ø¨Ù‡ Hugging Face\n# !huggingface-cli login --token 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\n\n# # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±\n# model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_dir)\n# tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n\n# # Ø¢Ù¾Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ù‡ Hugging Face\n# model.push_to_hub(repo_name)\n# tokenizer.push_to_hub(repo_name)\n","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n\nToken is valid (permission: write).\n\nYour token has been saved to /home/zeus/.cache/huggingface/token\n\nLogin successful\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c19840e3d4dc4fe4ad95e754ac09cd4e","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72917b2db274420388ebea5e87afdc8a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","source":"# Upload the model to Hugging Face Hub\n!huggingface-cli login --token {\"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\"}\nmodel.push_to_hub(\"long_t5_inference_base_line_25june\")\ntokenizer.push_to_hub(\"long_t5_inference_base_line_25june\")\n# !huggingface-cli repo create \"long\"  {\"Y\"}\n# !huggingface-cli upload output_dir ","metadata":{"id":"0qANgkug5o53","execution":{"iopub.status.busy":"2024-06-25T20:31:20.735748Z","iopub.execute_input":"2024-06-25T20:31:20.736161Z","iopub.status.idle":"2024-06-25T20:32:12.739005Z","shell.execute_reply.started":"2024-06-25T20:31:20.736130Z","shell.execute_reply":"2024-06-25T20:32:12.737793Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8159f683a04c8f884742543a9ec076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cfcad50fcc14a839170c5c9e55479b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76cad861c7c54ce6bb52eb12558915fb"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AliGhiasvand86/long_t5_inference_base_line_25june/commit/50c079ea649f46b85ad4ab25ec4b443ca04d2e15', commit_message='Upload tokenizer', commit_description='', oid='50c079ea649f46b85ad4ab25ec4b443ca04d2e15', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndef inference(model_path, input_text):\n    # Determine device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n\n    # Tokenize input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n\n    # Generate output\n    outputs = model.generate(**inputs, max_length=1024)\n\n    # Decode and return the result\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Constants\nmodel_path = \"AliGhiasvand86/long_t5_inference_base_line_25june\"\noutput_csv_path = 'AliGhiasvand86/long_t5_inference_base_line_25june'\n\n\n\n# Initialize empty lists for inputs and outputs\ninputs = []\noutputs = []\n\n# Iterate over each row and each score type to create inputs and get outputs\nfor idx, example in enumerate(test_data):\n    print(idx)\n    input_text = f\"{example['prompt']}\\n{example['essay']}\"\n    output_text = inference(model_path, input_text)\n    inputs.append(input_text)\n    outputs.append(output_text)\n\n# Create a new DataFrame with inputs and outputs\nresult_df = pd.DataFrame({'inputs': inputs, 'outputs': outputs})\n\n# Save the results to a CSV file\nresult_df.to_csv(output_csv_path, index=False)\nprint(f\"Inference completed and results saved to {output_csv_path}\")\nprint(\"HIIIIIIIIIIIIIIII\")\n# Check the first input in the saved CSV\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:34:35.705477Z","iopub.execute_input":"2024-06-25T20:34:35.706495Z","iopub.status.idle":"2024-06-25T20:49:36.351736Z","shell.execute_reply.started":"2024-06-25T20:34:35.706453Z","shell.execute_reply":"2024-06-25T20:49:36.350163Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d40eedb1a14a7da665ebaac59b8101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7bcc572b0674c6bb023368a8f9c61f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c69d7ede0e94ebfb8ddb8092afb85c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4de92495154d7e8e090bb3ebbca7ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/900 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"452f888ef19d40e385f2764516f5b124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e50b49d8b2748deba5ef54a08109205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d34078194e846548d52b27e7ada9402"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1006: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs})\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference completed and results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHIIIIIIIIIIIIIIII\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'AliGhiasvand86'"],"ename":"OSError","evalue":"Cannot save file into a non-existent directory: 'AliGhiasvand86'","output_type":"error"}]},{"cell_type":"code","source":"print(result_df[\"outputs\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:06:07.935676Z","iopub.execute_input":"2024-06-25T21:06:07.936053Z","iopub.status.idle":"2024-06-25T21:06:07.947578Z","shell.execute_reply.started":"2024-06-25T21:06:07.936023Z","shell.execute_reply":"2024-06-25T21:06:07.946354Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"0      content 4.0, organization 3.5, language 3.5\n1      content 4.0, organization 3.5, language 3.5\n2      content 4.0, organization 4.0, language 4.0\n3      content 4.0, organization 4.0, language 3.5\n4      content 4.5, organization 4.5, language 4.5\n                          ...                     \n391    content 3.0, organization 3.0, language 3.0\n392    content 3.5, organization 3.5, language 3.5\n393    content 3.0, organization 3.0, language 3.0\n394    content 5.0, organization 4.5, language 4.5\n395    content 4.0, organization 4.0, language 3.5\nName: outputs, Length: 396, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\npattern = r\"content (\\d+\\.\\d+), organization (\\d+\\.\\d+), language (\\d+\\.\\d+)\"\n\n# Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø³Ú©ÙˆØ±Ù‡Ø§\ncontent_scores_inference = []\norganization_scores_inference = []\nlanguage_scores_inference = []\n\n# Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø±Ø¯ÛŒÙ Ø§Ø² Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ú©ÙˆØ±Ù‡Ø§\nfor index, row in result_df.iterrows():\n    input_text = row['outputs']\n    \n    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ú©ÙˆØ±Ù‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯Ùˆ\n    match = re.match(pattern, input_text)\n    \n    if match:\n        content_score = float(match.group(1))\n        organization_score = float(match.group(2))\n        language_score = float(match.group(3))\n        \n        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³Ú©ÙˆØ±Ù‡Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§\n        content_scores_inference.append(content_score)\n        organization_scores_inference.append(organization_score)\n        language_scores_inference.append(language_score)\n        \n    else:\n        print(f\"Row {index + 1}: ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.\")\n\n# Ù†Ù…Ø§ÛŒØ´ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©ÙˆØ±Ù‡Ø§\nprint(\"Content Scores:\", content_scores_inference)\nprint(\"Organization Scores:\", organization_scores_inference)\nprint(\"Language Scores:\", language_scores_inference)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:08:17.448929Z","iopub.execute_input":"2024-06-25T21:08:17.449734Z","iopub.status.idle":"2024-06-25T21:08:17.489402Z","shell.execute_reply.started":"2024-06-25T21:08:17.449698Z","shell.execute_reply":"2024-06-25T21:08:17.488358Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Content Scores: [4.0, 4.0, 4.0, 4.0, 4.5, 4.5, 4.0, 4.5, 4.0, 2.5, 4.0, 4.5, 4.5, 4.5, 4.5, 3.5, 4.0, 4.5, 5.0, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 2.5, 4.5, 4.5, 4.5, 4.0, 4.5, 5.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.5, 3.0, 3.0, 4.5, 3.5, 3.5, 3.0, 3.5, 4.5, 3.0, 4.0, 4.0, 3.5, 3.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.5, 3.0, 2.5, 4.0, 4.0, 3.0, 4.0, 3.5, 4.0, 3.0, 2.5, 4.0, 4.5, 2.5, 3.5, 4.5, 4.0, 4.0, 4.0, 4.0, 2.5, 4.5, 3.5, 3.0, 3.5, 4.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.5, 4.0, 4.5, 4.5, 4.0, 3.5, 4.0, 5.0, 4.5, 4.5, 4.0, 4.0, 4.0, 3.5, 3.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 3.0, 3.5, 3.5, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.5, 2.5, 4.5, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 3.5, 3.0, 4.0, 4.5, 4.0, 4.5, 3.5, 4.5, 4.0, 3.0, 4.5, 4.0, 3.5, 4.5, 4.5, 3.5, 2.0, 2.5, 4.5, 3.5, 4.0, 4.5, 4.0, 2.5, 4.0, 4.0, 3.5, 4.5, 4.0, 4.0, 4.0, 4.5, 3.5, 5.0, 4.0, 3.0, 4.5, 3.5, 4.5, 4.0, 3.5, 4.5, 5.0, 4.0, 4.0, 4.0, 4.5, 3.5, 5.0, 4.5, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.5, 3.0, 4.0, 4.0, 4.0, 4.0, 4.5, 5.0, 3.0, 4.5, 4.0, 4.5, 4.0, 4.5, 4.5, 4.0, 3.5, 4.0, 3.0, 4.0, 4.0, 4.5, 4.0, 4.0, 3.0, 4.5, 4.0, 4.5, 4.5, 4.0, 5.0, 3.0, 4.5, 5.0, 4.0, 3.5, 4.0, 3.0, 4.5, 3.0, 4.5, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.5, 5.0, 3.5, 3.0, 3.5, 4.0, 4.0, 3.5, 4.5, 4.0, 4.0, 2.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.5, 2.5, 4.0, 4.0, 4.0, 3.0, 4.0, 4.5, 4.0, 5.0, 4.5, 4.5, 4.5, 5.0, 4.0, 4.0, 5.0, 5.0, 3.5, 5.0, 2.5, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 3.5, 5.0, 4.0, 2.0, 4.0, 4.0, 3.5, 3.0, 5.0, 4.0, 3.0, 4.0, 4.5, 4.5, 4.5, 4.0, 4.0, 4.5, 4.0, 3.0, 4.0, 4.5, 3.0, 4.0, 4.0, 4.5, 4.0, 3.5, 5.0, 3.5, 3.5, 4.5, 4.0, 4.5, 4.0, 4.0, 4.0, 3.0, 3.0, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 2.5, 4.0, 4.0, 3.0, 5.0, 3.5, 3.5, 5.0, 4.5, 5.0, 4.0, 3.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.5, 3.5, 3.0, 3.5, 2.0, 3.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.5, 4.0, 4.5, 2.5, 3.5, 3.5, 4.0, 4.0, 5.0, 4.0, 3.5, 4.0, 4.5, 3.0, 4.0, 4.5, 3.0, 3.5, 3.0, 5.0, 4.0]\nOrganization Scores: [3.5, 3.5, 4.0, 4.0, 4.5, 4.5, 4.0, 4.5, 4.0, 2.5, 4.0, 4.5, 4.5, 4.5, 4.5, 3.5, 4.0, 4.5, 5.0, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 2.5, 4.5, 4.5, 4.5, 4.0, 4.5, 5.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.5, 4.0, 4.5, 3.0, 3.0, 4.5, 3.5, 3.5, 3.0, 3.5, 4.5, 3.0, 3.5, 4.0, 3.5, 3.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.5, 3.0, 2.5, 4.0, 4.0, 3.0, 4.0, 3.5, 4.0, 3.0, 2.5, 4.0, 4.5, 2.5, 3.5, 4.5, 4.0, 4.0, 4.0, 4.0, 2.5, 4.5, 3.5, 3.0, 3.5, 4.0, 5.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 4.5, 4.0, 4.5, 4.5, 4.0, 3.5, 4.0, 4.5, 4.5, 4.5, 4.0, 4.0, 4.0, 3.5, 3.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 3.0, 3.5, 3.5, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.0, 4.5, 2.5, 4.5, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 3.5, 3.0, 4.0, 4.5, 4.0, 4.5, 3.5, 4.5, 3.5, 3.0, 4.5, 4.0, 3.5, 4.5, 4.5, 3.5, 2.0, 2.5, 4.5, 3.5, 4.0, 4.5, 4.0, 2.5, 4.0, 4.0, 3.5, 4.5, 3.5, 4.0, 4.0, 4.5, 3.5, 4.5, 4.0, 3.0, 4.5, 3.5, 4.5, 4.0, 3.5, 4.5, 4.5, 4.0, 4.0, 4.0, 4.5, 3.5, 4.5, 4.5, 4.0, 4.0, 3.5, 3.5, 4.0, 4.0, 4.0, 4.5, 3.0, 4.0, 4.0, 4.0, 4.0, 4.5, 5.0, 3.0, 4.5, 4.0, 4.5, 4.0, 4.5, 4.5, 4.0, 3.5, 4.0, 3.0, 4.0, 4.0, 4.5, 4.0, 4.0, 3.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 3.0, 4.5, 4.5, 4.0, 3.5, 4.0, 3.0, 4.5, 3.0, 4.5, 4.0, 4.0, 4.0, 3.5, 4.0, 4.0, 4.5, 5.0, 3.5, 3.0, 3.5, 4.0, 3.5, 3.5, 4.5, 4.0, 4.0, 2.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.0, 4.0, 4.0, 4.5, 2.5, 4.0, 4.0, 4.0, 3.0, 4.0, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 5.0, 4.0, 4.0, 4.5, 5.0, 3.5, 4.5, 2.5, 4.0, 4.0, 3.0, 4.0, 4.0, 4.0, 3.5, 5.0, 4.0, 2.0, 4.0, 4.0, 3.5, 3.0, 4.5, 4.0, 3.0, 4.0, 4.5, 4.5, 4.5, 4.0, 4.0, 4.5, 4.0, 3.0, 4.0, 4.5, 3.0, 4.0, 4.0, 4.5, 4.0, 3.5, 4.5, 3.5, 3.5, 4.5, 4.0, 4.5, 4.0, 4.0, 4.0, 3.0, 3.0, 3.5, 4.0, 4.0, 3.5, 4.0, 4.0, 2.5, 4.0, 3.5, 3.0, 5.0, 3.5, 3.5, 5.0, 4.5, 5.0, 3.5, 3.0, 3.5, 4.0, 4.0, 4.0, 4.0, 3.5, 3.5, 3.0, 3.5, 2.0, 3.5, 4.0, 4.0, 4.5, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.5, 4.0, 4.5, 4.0, 4.5, 2.5, 3.5, 3.5, 4.0, 4.0, 5.0, 4.0, 3.5, 4.0, 4.5, 3.0, 4.0, 4.5, 3.0, 3.5, 3.0, 4.5, 4.0]\nLanguage Scores: [3.5, 3.5, 4.0, 3.5, 4.5, 4.5, 3.5, 4.5, 3.5, 2.5, 4.0, 4.5, 4.5, 4.5, 4.5, 3.5, 3.5, 4.5, 4.5, 4.0, 3.5, 4.0, 4.0, 3.5, 3.5, 2.5, 4.5, 4.5, 4.5, 4.0, 4.5, 4.5, 3.5, 4.5, 3.5, 3.5, 4.5, 3.5, 3.5, 4.5, 4.0, 4.0, 4.5, 4.0, 4.5, 3.0, 3.0, 4.5, 3.5, 3.5, 2.5, 3.5, 4.5, 2.5, 3.5, 4.0, 3.5, 3.0, 4.0, 3.5, 3.5, 3.5, 3.5, 3.5, 4.5, 3.0, 2.5, 4.0, 3.5, 3.0, 3.5, 3.5, 4.0, 3.0, 2.5, 4.0, 4.5, 2.5, 3.5, 4.5, 4.0, 3.5, 4.0, 3.5, 2.5, 4.5, 3.5, 3.0, 3.5, 4.0, 4.5, 4.0, 3.5, 3.0, 3.5, 3.5, 4.0, 4.5, 3.5, 4.5, 4.5, 3.5, 3.5, 3.5, 4.5, 4.5, 4.5, 4.0, 3.5, 3.5, 3.5, 3.0, 4.0, 3.5, 3.5, 3.5, 4.0, 4.0, 3.5, 3.5, 3.5, 3.0, 3.5, 3.5, 3.5, 4.0, 3.5, 3.5, 4.0, 3.5, 4.0, 4.0, 3.5, 3.5, 4.0, 3.5, 4.5, 2.5, 4.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.0, 3.5, 4.5, 3.5, 4.5, 3.5, 4.5, 3.5, 3.0, 4.5, 4.0, 3.5, 4.5, 4.5, 3.5, 2.0, 2.5, 4.5, 3.5, 4.0, 4.5, 3.5, 2.5, 4.0, 3.5, 3.5, 4.5, 3.5, 3.5, 4.0, 4.5, 3.5, 4.5, 3.5, 3.0, 4.5, 3.5, 4.5, 3.5, 3.5, 4.5, 4.5, 3.5, 4.0, 3.5, 4.5, 3.5, 4.5, 4.5, 4.0, 4.0, 3.5, 3.5, 3.5, 4.0, 3.5, 4.5, 3.0, 4.0, 4.0, 3.5, 3.5, 4.5, 4.5, 3.0, 4.5, 3.5, 4.5, 4.0, 4.5, 4.5, 3.5, 3.5, 3.5, 3.0, 3.5, 4.0, 4.5, 4.0, 3.5, 3.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 3.0, 4.5, 4.5, 4.0, 3.5, 4.0, 3.0, 4.5, 3.0, 4.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 4.5, 4.5, 3.5, 3.0, 3.5, 3.5, 3.5, 3.5, 4.5, 4.0, 4.0, 2.0, 3.5, 3.5, 3.5, 3.5, 3.5, 3.0, 3.5, 4.0, 4.5, 2.5, 3.5, 3.5, 3.5, 2.5, 3.5, 4.5, 3.5, 4.5, 4.5, 4.5, 4.5, 4.5, 3.5, 3.5, 4.5, 4.5, 3.5, 4.5, 2.5, 3.5, 3.5, 3.0, 3.5, 3.5, 3.5, 3.5, 4.5, 3.5, 2.0, 4.0, 3.5, 3.5, 3.0, 4.5, 4.0, 3.0, 3.5, 4.5, 4.5, 4.5, 3.5, 3.5, 4.5, 3.5, 3.0, 4.0, 4.5, 3.0, 3.5, 4.0, 4.5, 3.5, 3.5, 4.5, 3.5, 3.5, 4.5, 3.5, 4.5, 3.5, 3.5, 3.5, 3.0, 3.0, 3.5, 3.5, 3.5, 3.5, 4.0, 3.5, 2.5, 3.5, 3.5, 3.0, 4.5, 3.5, 3.5, 4.5, 4.5, 4.5, 3.5, 3.0, 3.5, 4.0, 3.5, 3.5, 4.0, 3.0, 3.5, 3.0, 3.5, 2.0, 3.5, 3.5, 3.5, 4.5, 3.5, 3.5, 4.0, 4.0, 4.0, 3.5, 3.5, 4.0, 4.5, 4.0, 4.5, 2.5, 3.5, 3.5, 4.0, 3.5, 4.5, 3.5, 3.5, 3.5, 4.5, 3.0, 3.5, 4.5, 3.0, 3.5, 3.0, 4.5, 3.5]\n","output_type":"stream"}]},{"cell_type":"code","source":"content_scores = test_df['content'].tolist()\norganization_score = test_df['organization'].tolist()\nlanguage_score = test_df['language'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:09:12.941327Z","iopub.execute_input":"2024-06-25T21:09:12.942292Z","iopub.status.idle":"2024-06-25T21:09:12.949203Z","shell.execute_reply.started":"2024-06-25T21:09:12.942246Z","shell.execute_reply":"2024-06-25T21:09:12.947619Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def transform_list_1(scores):\n    transformed_scores = []\n    for score in scores:\n        if score == 0:\n            transformed_scores.append(0)\n        elif score == 0.5:\n            transformed_scores.append(1)\n        elif score == 1:\n            transformed_scores.append(2)\n        elif score == 1.5:\n            transformed_scores.append(3)\n        elif score == 2:\n            transformed_scores.append(4)\n        elif score == 2.5:\n            transformed_scores.append(5)\n        elif score == 3:\n            transformed_scores.append(6)\n        elif score == 3.5:\n            transformed_scores.append(7)\n        elif score == 4:\n            transformed_scores.append(8)\n        elif score == 4.5:\n            transformed_scores.append(9)\n        elif score == 5:\n            transformed_scores.append(10)\n        else:\n            # Handle other cases if needed\n            pass\n    return transformed_scores\n\ndef transform_list_2(scores):\n    transformed_scores = []\n    for score in scores:\n        if score == 0:\n            transformed_scores.append(0)\n        elif score == 0.5:\n            transformed_scores.append(1)\n        elif score == 1:\n            transformed_scores.append(2)\n        elif score == 1.5:\n            transformed_scores.append(3)\n        elif score == 2:\n            transformed_scores.append(4)\n        elif score == 2.5:\n            transformed_scores.append(5)\n        elif score == 3:\n            transformed_scores.append(6)\n        elif score == 3.5:\n            transformed_scores.append(7)\n        elif score == 4:\n            transformed_scores.append(8)\n        elif score == 4.5:\n            transformed_scores.append(9)\n        elif score == 5:\n            transformed_scores.append(10)\n        else:\n            # Handle other cases if needed\n            pass\n    return transformed_scores\n\n# Example usage:\n\n\ntransformed_scores_1 = transform_list_1(language_score)\ntransformed_scores_2 = transform_list_2(language_scores_inference)\n\nprint(\"Transformed Content Scores:\", transformed_scores_1)\nprint(\"Transformed Content Score Inference:\", transformed_scores_2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:14:15.281086Z","iopub.execute_input":"2024-06-25T21:14:15.281484Z","iopub.status.idle":"2024-06-25T21:14:15.298556Z","shell.execute_reply.started":"2024-06-25T21:14:15.281454Z","shell.execute_reply":"2024-06-25T21:14:15.297633Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Transformed Content Scores: [6, 7, 7, 6, 9, 9, 6, 8, 8, 5, 7, 7, 8, 8, 8, 5, 6, 8, 7, 9, 8, 8, 7, 6, 5, 6, 8, 8, 7, 8, 9, 9, 6, 9, 4, 6, 9, 8, 8, 8, 7, 7, 8, 6, 9, 4, 7, 7, 4, 8, 5, 7, 9, 4, 6, 8, 6, 3, 8, 5, 7, 6, 8, 5, 6, 5, 5, 8, 5, 5, 7, 6, 9, 6, 5, 8, 8, 5, 7, 9, 6, 7, 8, 7, 5, 7, 7, 5, 7, 8, 9, 7, 6, 6, 7, 5, 6, 6, 7, 9, 8, 6, 6, 6, 8, 9, 9, 9, 4, 7, 6, 8, 8, 8, 4, 7, 7, 7, 4, 6, 8, 5, 6, 5, 7, 7, 8, 5, 8, 8, 7, 7, 6, 9, 6, 7, 9, 4, 8, 7, 6, 7, 7, 5, 6, 5, 7, 5, 7, 5, 8, 5, 8, 5, 5, 8, 7, 4, 8, 9, 5, 4, 5, 8, 5, 7, 7, 6, 5, 8, 5, 3, 9, 4, 4, 6, 8, 7, 9, 6, 6, 8, 6, 8, 7, 7, 8, 8, 5, 8, 5, 8, 5, 7, 7, 8, 7, 6, 6, 9, 7, 6, 9, 4, 6, 8, 7, 6, 9, 8, 6, 7, 6, 8, 7, 8, 8, 5, 5, 8, 6, 7, 8, 8, 7, 5, 6, 8, 8, 7, 7, 8, 9, 2, 9, 8, 8, 7, 7, 4, 8, 6, 8, 6, 7, 8, 6, 6, 5, 8, 7, 6, 7, 7, 7, 5, 6, 9, 7, 7, 4, 4, 7, 6, 7, 8, 6, 9, 8, 8, 3, 7, 5, 6, 7, 4, 8, 7, 8, 8, 8, 9, 6, 8, 8, 6, 8, 4, 9, 6, 6, 6, 5, 6, 7, 5, 7, 5, 8, 7, 6, 8, 6, 7, 7, 8, 5, 6, 7, 7, 9, 8, 6, 8, 7, 3, 7, 8, 3, 6, 6, 7, 7, 5, 7, 6, 8, 8, 7, 8, 7, 6, 7, 5, 5, 7, 6, 5, 8, 6, 6, 5, 7, 7, 5, 5, 7, 5, 8, 9, 8, 6, 4, 6, 7, 2, 7, 8, 6, 6, 4, 6, 5, 6, 6, 6, 9, 6, 5, 7, 7, 7, 6, 7, 8, 8, 7, 9, 4, 6, 7, 10, 6, 9, 7, 6, 7, 7, 6, 7, 7, 6, 3, 4, 7, 5]\nTransformed Content Score Inference: [7, 7, 8, 7, 9, 9, 7, 9, 7, 5, 8, 9, 9, 9, 9, 7, 7, 9, 9, 8, 7, 8, 8, 7, 7, 5, 9, 9, 9, 8, 9, 9, 7, 9, 7, 7, 9, 7, 7, 9, 8, 8, 9, 8, 9, 6, 6, 9, 7, 7, 5, 7, 9, 5, 7, 8, 7, 6, 8, 7, 7, 7, 7, 7, 9, 6, 5, 8, 7, 6, 7, 7, 8, 6, 5, 8, 9, 5, 7, 9, 8, 7, 8, 7, 5, 9, 7, 6, 7, 8, 9, 8, 7, 6, 7, 7, 8, 9, 7, 9, 9, 7, 7, 7, 9, 9, 9, 8, 7, 7, 7, 6, 8, 7, 7, 7, 8, 8, 7, 7, 7, 6, 7, 7, 7, 8, 7, 7, 8, 7, 8, 8, 7, 7, 8, 7, 9, 5, 9, 7, 7, 7, 7, 7, 7, 7, 6, 7, 9, 7, 9, 7, 9, 7, 6, 9, 8, 7, 9, 9, 7, 4, 5, 9, 7, 8, 9, 7, 5, 8, 7, 7, 9, 7, 7, 8, 9, 7, 9, 7, 6, 9, 7, 9, 7, 7, 9, 9, 7, 8, 7, 9, 7, 9, 9, 8, 8, 7, 7, 7, 8, 7, 9, 6, 8, 8, 7, 7, 9, 9, 6, 9, 7, 9, 8, 9, 9, 7, 7, 7, 6, 7, 8, 9, 8, 7, 6, 9, 8, 9, 9, 8, 9, 6, 9, 9, 8, 7, 8, 6, 9, 6, 9, 7, 7, 7, 7, 7, 7, 9, 9, 7, 6, 7, 7, 7, 7, 9, 8, 8, 4, 7, 7, 7, 7, 7, 6, 7, 8, 9, 5, 7, 7, 7, 5, 7, 9, 7, 9, 9, 9, 9, 9, 7, 7, 9, 9, 7, 9, 5, 7, 7, 6, 7, 7, 7, 7, 9, 7, 4, 8, 7, 7, 6, 9, 8, 6, 7, 9, 9, 9, 7, 7, 9, 7, 6, 8, 9, 6, 7, 8, 9, 7, 7, 9, 7, 7, 9, 7, 9, 7, 7, 7, 6, 6, 7, 7, 7, 7, 8, 7, 5, 7, 7, 6, 9, 7, 7, 9, 9, 9, 7, 6, 7, 8, 7, 7, 8, 6, 7, 6, 7, 4, 7, 7, 7, 9, 7, 7, 8, 8, 8, 7, 7, 8, 9, 8, 9, 5, 7, 7, 8, 7, 9, 7, 7, 7, 9, 6, 7, 9, 6, 7, 6, 9, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\nprint(\"QWK: \", cohen_kappa_score(transformed_scores_2, transformed_scores_1, weights=\"quadratic\"))","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:14:17.530431Z","iopub.execute_input":"2024-06-25T21:14:17.530799Z","iopub.status.idle":"2024-06-25T21:14:17.542021Z","shell.execute_reply.started":"2024-06-25T21:14:17.530771Z","shell.execute_reply":"2024-06-25T21:14:17.540856Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"QWK:  0.5352083619859717\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:15:25.197293Z","iopub.execute_input":"2024-06-25T21:15:25.197663Z","iopub.status.idle":"2024-06-25T21:15:25.204814Z","shell.execute_reply.started":"2024-06-25T21:15:25.197634Z","shell.execute_reply":"2024-06-25T21:15:25.203738Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"total_dataset = [x + y + z for x, y, z in zip(content_scores, organization_score, language_score)]\n\nprint(total_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:17:12.145889Z","iopub.execute_input":"2024-06-25T21:17:12.146800Z","iopub.status.idle":"2024-06-25T21:17:12.154485Z","shell.execute_reply.started":"2024-06-25T21:17:12.146765Z","shell.execute_reply":"2024-06-25T21:17:12.153369Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"[10.0, 11.5, 10.5, 10.0, 14.5, 13.0, 9.0, 12.0, 12.0, 7.5, 10.5, 11.0, 12.0, 12.5, 12.0, 8.0, 10.0, 12.0, 10.5, 14.5, 12.0, 12.5, 11.5, 9.0, 7.5, 7.5, 12.0, 12.5, 13.0, 12.0, 14.0, 13.0, 9.0, 13.0, 7.5, 8.0, 13.0, 12.0, 11.5, 14.0, 11.0, 11.5, 12.0, 10.5, 13.0, 5.5, 10.5, 11.5, 8.0, 11.0, 8.0, 9.5, 14.5, 6.5, 9.5, 12.0, 9.0, 4.5, 13.0, 8.5, 10.0, 9.5, 12.0, 8.0, 10.0, 7.5, 8.0, 13.0, 8.5, 7.5, 10.5, 9.0, 13.5, 9.0, 8.5, 12.0, 12.0, 7.5, 11.0, 14.5, 9.0, 11.0, 12.0, 11.5, 8.0, 11.5, 11.0, 6.5, 9.5, 12.5, 14.5, 11.5, 11.0, 9.0, 9.5, 7.0, 9.0, 10.5, 10.5, 13.5, 12.0, 10.5, 9.5, 10.0, 11.5, 13.5, 13.5, 14.0, 6.5, 12.0, 10.5, 12.0, 12.0, 11.5, 8.5, 11.5, 11.0, 12.0, 5.5, 9.5, 12.5, 3.5, 9.0, 7.5, 11.0, 12.5, 11.5, 8.0, 11.0, 12.5, 11.0, 11.0, 8.5, 14.0, 10.0, 11.0, 13.5, 5.5, 11.5, 9.5, 9.5, 10.0, 9.5, 8.5, 8.0, 7.5, 10.5, 8.5, 11.0, 8.0, 13.0, 10.0, 12.5, 8.0, 7.0, 13.5, 10.0, 6.5, 12.0, 13.5, 9.0, 5.5, 8.0, 13.0, 8.0, 10.0, 11.0, 10.5, 8.5, 12.0, 8.5, 5.5, 13.0, 6.0, 6.5, 9.5, 13.5, 9.5, 13.5, 9.0, 9.5, 12.5, 8.0, 12.0, 10.5, 10.0, 12.0, 12.5, 8.5, 12.5, 9.0, 12.0, 6.5, 11.5, 11.0, 13.0, 13.0, 10.5, 10.5, 14.0, 11.0, 9.5, 12.5, 6.5, 9.5, 12.5, 12.0, 9.5, 13.5, 12.0, 9.5, 11.0, 10.5, 12.5, 10.0, 12.0, 13.0, 6.0, 9.5, 12.5, 9.5, 11.0, 13.0, 13.5, 12.5, 7.5, 8.5, 12.0, 12.0, 11.5, 10.5, 12.0, 14.0, 4.5, 13.5, 13.0, 13.0, 12.0, 10.0, 7.5, 12.5, 9.0, 12.0, 10.5, 11.0, 13.0, 10.0, 10.0, 8.0, 12.0, 12.0, 8.5, 8.0, 10.5, 9.5, 9.5, 9.0, 12.5, 11.0, 11.5, 5.5, 5.5, 13.0, 10.0, 10.5, 11.5, 9.0, 13.5, 13.0, 11.5, 5.0, 10.5, 8.0, 10.5, 10.0, 8.5, 13.0, 11.0, 12.0, 12.0, 12.0, 13.5, 10.0, 10.5, 12.5, 8.5, 13.0, 6.5, 13.5, 9.0, 9.0, 9.5, 4.0, 11.0, 11.0, 8.5, 10.0, 11.5, 10.5, 9.5, 10.5, 13.5, 8.5, 10.5, 12.0, 13.0, 8.0, 9.5, 11.5, 10.5, 13.5, 12.0, 9.0, 12.0, 9.0, 4.0, 10.5, 13.0, 6.0, 9.5, 10.0, 12.5, 11.5, 8.5, 12.5, 8.0, 7.5, 12.0, 12.5, 12.0, 11.0, 9.0, 8.5, 9.5, 8.0, 8.0, 9.5, 7.0, 11.0, 9.0, 9.0, 8.0, 11.0, 12.0, 7.0, 10.0, 8.5, 9.5, 12.5, 12.0, 12.5, 9.0, 7.5, 8.0, 12.5, 5.0, 10.0, 12.5, 8.5, 7.5, 5.0, 8.0, 8.0, 9.5, 10.5, 9.0, 13.0, 8.0, 8.5, 10.5, 11.0, 11.0, 9.0, 8.5, 11.0, 12.0, 11.0, 13.0, 5.5, 9.0, 9.5, 14.5, 9.0, 13.0, 10.5, 10.0, 10.5, 10.5, 9.5, 11.0, 11.0, 9.0, 6.0, 5.0, 11.5, 8.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"total_inference = [x + y + z for x, y, z in zip(content_scores_inference, organization_scores_inference, language_scores_inference)]\n\nprint(total_inference)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:17:12.733626Z","iopub.execute_input":"2024-06-25T21:17:12.734323Z","iopub.status.idle":"2024-06-25T21:17:12.742048Z","shell.execute_reply.started":"2024-06-25T21:17:12.734293Z","shell.execute_reply":"2024-06-25T21:17:12.740500Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[11.0, 11.0, 12.0, 11.5, 13.5, 13.5, 11.5, 13.5, 11.5, 7.5, 12.0, 13.5, 13.5, 13.5, 13.5, 10.5, 11.5, 13.5, 14.5, 12.0, 11.5, 12.0, 12.0, 10.5, 11.5, 7.5, 13.5, 13.5, 13.5, 12.0, 13.5, 14.5, 11.5, 13.5, 11.5, 11.5, 13.5, 11.5, 11.5, 13.5, 12.0, 12.0, 13.5, 12.0, 13.5, 9.0, 9.0, 13.5, 10.5, 10.5, 8.5, 10.5, 13.5, 8.5, 11.0, 12.0, 10.5, 9.0, 12.0, 11.5, 10.5, 11.5, 11.5, 11.5, 13.5, 9.0, 7.5, 12.0, 11.5, 9.0, 11.5, 10.5, 12.0, 9.0, 7.5, 12.0, 13.5, 7.5, 10.5, 13.5, 12.0, 11.5, 12.0, 11.5, 7.5, 13.5, 10.5, 9.0, 10.5, 12.0, 14.5, 12.0, 11.5, 9.0, 11.5, 11.5, 12.0, 13.5, 11.5, 13.5, 13.5, 11.5, 10.5, 11.5, 14.0, 13.5, 13.5, 12.0, 11.5, 11.5, 10.5, 9.0, 12.0, 11.5, 11.5, 10.5, 12.0, 12.0, 10.5, 11.5, 11.5, 9.0, 10.5, 10.5, 10.5, 12.0, 11.5, 10.5, 12.0, 11.5, 12.0, 12.0, 10.5, 11.5, 12.0, 11.5, 13.5, 7.5, 13.5, 11.5, 11.5, 11.5, 11.5, 10.5, 11.5, 10.5, 9.0, 11.5, 13.5, 11.5, 13.5, 10.5, 13.5, 11.0, 9.0, 13.5, 12.0, 10.5, 13.5, 13.5, 10.5, 6.0, 7.5, 13.5, 10.5, 12.0, 13.5, 11.5, 7.5, 12.0, 11.5, 10.5, 13.5, 11.0, 11.5, 12.0, 13.5, 10.5, 14.0, 11.5, 9.0, 13.5, 10.5, 13.5, 11.5, 10.5, 13.5, 14.0, 11.5, 12.0, 11.5, 13.5, 10.5, 14.0, 13.5, 12.0, 12.0, 11.0, 10.5, 11.5, 12.0, 11.5, 13.5, 9.0, 12.0, 12.0, 11.5, 11.5, 13.5, 14.5, 9.0, 13.5, 11.5, 13.5, 12.0, 13.5, 13.5, 11.5, 10.5, 11.5, 9.0, 11.5, 12.0, 13.5, 12.0, 11.5, 9.0, 13.5, 12.0, 13.5, 13.5, 12.0, 14.0, 9.0, 13.5, 14.0, 12.0, 10.5, 12.0, 9.0, 13.5, 9.0, 13.5, 11.5, 11.5, 11.5, 10.5, 11.5, 11.5, 13.5, 14.5, 10.5, 9.0, 10.5, 11.5, 11.0, 10.5, 13.5, 12.0, 12.0, 6.0, 10.5, 11.5, 11.5, 11.5, 11.5, 9.0, 11.5, 12.0, 13.5, 7.5, 11.5, 11.5, 11.5, 8.5, 11.5, 13.5, 11.5, 14.0, 13.5, 13.5, 13.5, 14.5, 11.5, 11.5, 14.0, 14.5, 10.5, 14.0, 7.5, 11.5, 11.5, 9.0, 11.5, 11.5, 11.5, 10.5, 14.5, 11.5, 6.0, 12.0, 11.5, 10.5, 9.0, 14.0, 12.0, 9.0, 11.5, 13.5, 13.5, 13.5, 11.5, 11.5, 13.5, 11.5, 9.0, 12.0, 13.5, 9.0, 11.5, 12.0, 13.5, 11.5, 10.5, 14.0, 10.5, 10.5, 13.5, 11.5, 13.5, 11.5, 11.5, 11.5, 9.0, 9.0, 10.5, 11.5, 11.5, 10.5, 12.0, 11.5, 7.5, 11.5, 11.0, 9.0, 14.5, 10.5, 10.5, 14.5, 13.5, 14.5, 11.0, 9.0, 10.5, 12.0, 11.5, 11.5, 12.0, 10.0, 10.5, 9.0, 10.5, 6.0, 10.5, 11.5, 11.5, 13.5, 11.5, 11.5, 12.0, 12.0, 12.0, 11.5, 10.5, 12.0, 13.5, 12.0, 13.5, 7.5, 10.5, 10.5, 12.0, 11.5, 14.5, 11.5, 10.5, 11.5, 13.5, 9.0, 11.5, 13.5, 9.0, 10.5, 9.0, 14.0, 11.5]\n","output_type":"stream"}]},{"cell_type":"code","source":"def transform_list_1(scores):\n    transformed_scores = []\n    for score in scores:\n        if score == 0:\n            transformed_scores.append(0)\n        elif score == 0.5:\n          transformed_scores.append(1)\n        elif score == 1:\n            transformed_scores.append(2)\n        elif score == 1.5:\n            transformed_scores.append(3)\n        elif score == 2:\n            transformed_scores.append(4)\n        elif score == 2.5:\n            transformed_scores.append(5)\n        elif score == 3:\n            transformed_scores.append(6)\n        elif score == 3.5:\n            transformed_scores.append(7)\n        elif score == 4:\n            transformed_scores.append(8)\n        elif score == 4.5:\n            transformed_scores.append(9)\n        elif score == 5:\n            transformed_scores.append(10)\n        elif score == 5.5:\n            transformed_scores.append(11)\n        elif score == 6:\n            transformed_scores.append(12)\n        elif score == 6.5:\n            transformed_scores.append(13)\n        elif score == 7:\n            transformed_scores.append(14)\n        elif score == 7.5:\n            transformed_scores.append(15)\n        elif score == 8:\n            transformed_scores.append(16)\n        elif score == 8.5:\n            transformed_scores.append(17)\n        elif score == 9:\n            transformed_scores.append(18)\n        elif score == 9.5:\n            transformed_scores.append(19)\n        elif score == 10:\n            transformed_scores.append(20)\n        elif score == 10.5:\n            transformed_scores.append(21)\n        elif score == 11:\n            transformed_scores.append(22)\n        elif score == 11.5:\n            transformed_scores.append(23)\n        elif score == 12:\n            transformed_scores.append(24)\n        elif score == 12.5:\n            transformed_scores.append(25)\n        elif score == 13:\n            transformed_scores.append(26)\n        elif score == 13.5:\n            transformed_scores.append(27)\n        elif score == 14:\n            transformed_scores.append(28)\n        elif score == 14.5:\n            transformed_scores.append(29)\n        elif score == 15:\n            transformed_scores.append(30)\n\n        else:\n            # Handle other cases if needed\n            pass\n    return transformed_scores\n\ndef transform_list_2(scores):\n    transformed_scores = []\n    for score in scores:\n        if score == 0:\n            transformed_scores.append(0)\n        elif score == 0.5:\n          transformed_scores.append(1)\n        elif score == 1:\n            transformed_scores.append(2)\n        elif score == 1.5:\n            transformed_scores.append(3)\n        elif score == 2:\n            transformed_scores.append(4)\n        elif score == 2.5:\n            transformed_scores.append(5)\n        elif score == 3:\n            transformed_scores.append(6)\n        elif score == 3.5:\n            transformed_scores.append(7)\n        elif score == 4:\n            transformed_scores.append(8)\n        elif score == 4.5:\n            transformed_scores.append(9)\n        elif score == 5:\n            transformed_scores.append(10)\n        elif score == 5.5:\n            transformed_scores.append(11)\n        elif score == 6:\n            transformed_scores.append(12)\n        elif score == 6.5:\n            transformed_scores.append(13)\n        elif score == 7:\n            transformed_scores.append(14)\n        elif score == 7.5:\n            transformed_scores.append(15)\n        elif score == 8:\n            transformed_scores.append(16)\n        elif score == 8.5:\n            transformed_scores.append(17)\n        elif score == 9:\n            transformed_scores.append(18)\n        elif score == 9.5:\n            transformed_scores.append(19)\n        elif score == 10:\n            transformed_scores.append(20)\n        elif score == 10.5:\n            transformed_scores.append(21)\n        elif score == 11:\n            transformed_scores.append(22)\n        elif score == 11.5:\n            transformed_scores.append(23)\n        elif score == 12:\n            transformed_scores.append(24)\n        elif score == 12.5:\n            transformed_scores.append(25)\n        elif score == 13:\n            transformed_scores.append(26)\n        elif score == 13.5:\n            transformed_scores.append(27)\n        elif score == 14:\n            transformed_scores.append(28)\n        elif score == 14.5:\n            transformed_scores.append(29)\n        elif score == 15:\n            transformed_scores.append(30)\n        else:\n            # Handle other cases if needed\n            pass\n    return transformed_scores\n\n# Example usage:\n\n\ntransformed_scores_1 = transform_list_1(total_dataset)\ntransformed_scores_2 = transform_list_2(total_inference)\n\nprint(\"Transformed Content Scores:\", transformed_scores_1)\nprint(\"Transformed Content Score Inference:\", transformed_scores_2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:17:22.586011Z","iopub.execute_input":"2024-06-25T21:17:22.586685Z","iopub.status.idle":"2024-06-25T21:17:22.618987Z","shell.execute_reply.started":"2024-06-25T21:17:22.586652Z","shell.execute_reply":"2024-06-25T21:17:22.617870Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Transformed Content Scores: [20, 23, 21, 20, 29, 26, 18, 24, 24, 15, 21, 22, 24, 25, 24, 16, 20, 24, 21, 29, 24, 25, 23, 18, 15, 15, 24, 25, 26, 24, 28, 26, 18, 26, 15, 16, 26, 24, 23, 28, 22, 23, 24, 21, 26, 11, 21, 23, 16, 22, 16, 19, 29, 13, 19, 24, 18, 9, 26, 17, 20, 19, 24, 16, 20, 15, 16, 26, 17, 15, 21, 18, 27, 18, 17, 24, 24, 15, 22, 29, 18, 22, 24, 23, 16, 23, 22, 13, 19, 25, 29, 23, 22, 18, 19, 14, 18, 21, 21, 27, 24, 21, 19, 20, 23, 27, 27, 28, 13, 24, 21, 24, 24, 23, 17, 23, 22, 24, 11, 19, 25, 7, 18, 15, 22, 25, 23, 16, 22, 25, 22, 22, 17, 28, 20, 22, 27, 11, 23, 19, 19, 20, 19, 17, 16, 15, 21, 17, 22, 16, 26, 20, 25, 16, 14, 27, 20, 13, 24, 27, 18, 11, 16, 26, 16, 20, 22, 21, 17, 24, 17, 11, 26, 12, 13, 19, 27, 19, 27, 18, 19, 25, 16, 24, 21, 20, 24, 25, 17, 25, 18, 24, 13, 23, 22, 26, 26, 21, 21, 28, 22, 19, 25, 13, 19, 25, 24, 19, 27, 24, 19, 22, 21, 25, 20, 24, 26, 12, 19, 25, 19, 22, 26, 27, 25, 15, 17, 24, 24, 23, 21, 24, 28, 9, 27, 26, 26, 24, 20, 15, 25, 18, 24, 21, 22, 26, 20, 20, 16, 24, 24, 17, 16, 21, 19, 19, 18, 25, 22, 23, 11, 11, 26, 20, 21, 23, 18, 27, 26, 23, 10, 21, 16, 21, 20, 17, 26, 22, 24, 24, 24, 27, 20, 21, 25, 17, 26, 13, 27, 18, 18, 19, 8, 22, 22, 17, 20, 23, 21, 19, 21, 27, 17, 21, 24, 26, 16, 19, 23, 21, 27, 24, 18, 24, 18, 8, 21, 26, 12, 19, 20, 25, 23, 17, 25, 16, 15, 24, 25, 24, 22, 18, 17, 19, 16, 16, 19, 14, 22, 18, 18, 16, 22, 24, 14, 20, 17, 19, 25, 24, 25, 18, 15, 16, 25, 10, 20, 25, 17, 15, 10, 16, 16, 19, 21, 18, 26, 16, 17, 21, 22, 22, 18, 17, 22, 24, 22, 26, 11, 18, 19, 29, 18, 26, 21, 20, 21, 21, 19, 22, 22, 18, 12, 10, 23, 16]\nTransformed Content Score Inference: [22, 22, 24, 23, 27, 27, 23, 27, 23, 15, 24, 27, 27, 27, 27, 21, 23, 27, 29, 24, 23, 24, 24, 21, 23, 15, 27, 27, 27, 24, 27, 29, 23, 27, 23, 23, 27, 23, 23, 27, 24, 24, 27, 24, 27, 18, 18, 27, 21, 21, 17, 21, 27, 17, 22, 24, 21, 18, 24, 23, 21, 23, 23, 23, 27, 18, 15, 24, 23, 18, 23, 21, 24, 18, 15, 24, 27, 15, 21, 27, 24, 23, 24, 23, 15, 27, 21, 18, 21, 24, 29, 24, 23, 18, 23, 23, 24, 27, 23, 27, 27, 23, 21, 23, 28, 27, 27, 24, 23, 23, 21, 18, 24, 23, 23, 21, 24, 24, 21, 23, 23, 18, 21, 21, 21, 24, 23, 21, 24, 23, 24, 24, 21, 23, 24, 23, 27, 15, 27, 23, 23, 23, 23, 21, 23, 21, 18, 23, 27, 23, 27, 21, 27, 22, 18, 27, 24, 21, 27, 27, 21, 12, 15, 27, 21, 24, 27, 23, 15, 24, 23, 21, 27, 22, 23, 24, 27, 21, 28, 23, 18, 27, 21, 27, 23, 21, 27, 28, 23, 24, 23, 27, 21, 28, 27, 24, 24, 22, 21, 23, 24, 23, 27, 18, 24, 24, 23, 23, 27, 29, 18, 27, 23, 27, 24, 27, 27, 23, 21, 23, 18, 23, 24, 27, 24, 23, 18, 27, 24, 27, 27, 24, 28, 18, 27, 28, 24, 21, 24, 18, 27, 18, 27, 23, 23, 23, 21, 23, 23, 27, 29, 21, 18, 21, 23, 22, 21, 27, 24, 24, 12, 21, 23, 23, 23, 23, 18, 23, 24, 27, 15, 23, 23, 23, 17, 23, 27, 23, 28, 27, 27, 27, 29, 23, 23, 28, 29, 21, 28, 15, 23, 23, 18, 23, 23, 23, 21, 29, 23, 12, 24, 23, 21, 18, 28, 24, 18, 23, 27, 27, 27, 23, 23, 27, 23, 18, 24, 27, 18, 23, 24, 27, 23, 21, 28, 21, 21, 27, 23, 27, 23, 23, 23, 18, 18, 21, 23, 23, 21, 24, 23, 15, 23, 22, 18, 29, 21, 21, 29, 27, 29, 22, 18, 21, 24, 23, 23, 24, 20, 21, 18, 21, 12, 21, 23, 23, 27, 23, 23, 24, 24, 24, 23, 21, 24, 27, 24, 27, 15, 21, 21, 24, 23, 29, 23, 21, 23, 27, 18, 23, 27, 18, 21, 18, 28, 23]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\nprint(\"QWK: \", cohen_kappa_score(transformed_scores_2, transformed_scores_1, weights=\"quadratic\"))","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:17:34.177793Z","iopub.execute_input":"2024-06-25T21:17:34.178205Z","iopub.status.idle":"2024-06-25T21:17:34.188834Z","shell.execute_reply.started":"2024-06-25T21:17:34.178176Z","shell.execute_reply":"2024-06-25T21:17:34.187571Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"QWK:  0.5595120529715554\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}